import re
import os
import time
import random
import json
import traceback
from datetime import datetime
from urllib.parse import urlencode
from scraper.base_scrapper import (
    FromDateScrapper,
    BypassCloudfareNoProxySpider
)
from scrapy import (
    Spider,
    Request,
    FormRequest,
    Selector
)


# Credentials
USERNAME = "NightCat"
PASSWORD = "2PK&fx2i%yL%FsIMwJaE5gfr"

# Topic Counter
TOPIC_START_COUNT = 100
TOPIC_END_COUNT = 200


class ExploitFromDateSpider(BypassCloudfareNoProxySpider):

    name = "exploit_from_date"

    # Url stuffs
    start_urls = ["https://forum.exploit.in/login"]
    unread_url = "https://forum.exploit.in/discover/unread/?&stream_read=all&" \
                 "stream_date_type=custom&stream_date_start=%s&stream_date_end=%s&" \
                 "stream_classes[IPS\\forums\\Topic]=1"
    unread_api = "https://forum.exploit.in/discover/unread/"

    # Payload stuff
    post_headers = {
        "Content-Type": "application/x-www-form-urlencoded"
    }
    pagination_payload = {
        "csrfKey": "",
        "before": "",
        "stream_read": "all",
        "stream_classes[IPS\\forums\Topic]": "Topics"
    }

    # Css stuffs
    thread_url_css = r".ipsType_break>a[href*=\/topic]::attr(href)"
    thread_stamp_css = r".ipsStreamItem::attr(data-timestamp)"
    csrf_key_css = r"[name=csrfKey]::attr(value)"
    current_page_css = r".ipsPagination_active>a::text"
    next_page_css = r".ipsPagination_next>a::attr(href)"

    # Regex stuff
    get_topic_id = re.compile(
        r"(?<=\/topic\/)\d*?(?=\/|$)",
        re.IGNORECASE
    )

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.output_path = kwargs.get("output_path")
        self.headers = {
            "User-Agent": self.custom_settings.get("DEFAULT_REQUEST_HEADERS")
        }
        self.post_headers.update(self.headers)

    def parse(self, response):
        yield FormRequest.from_response(
            response=response,
            formdata={
                "auth": USERNAME,
                "password": PASSWORD
            },
            headers=self.post_headers,
            callback=self.parse_unread
        )

    def parse_unread(self, response):
        yield Request(
            url=self.unread_url % (
                str(int(self.start_date.timestamp())),
                str(int(datetime.now().timestamp()))
            ),
            headers=self.headers,
            callback=self.parse_thread
        )

    def parse_thread(self, response, csrf_key=None):

        # Load csrf key
        if not csrf_key:
            csrf_key = response.css(self.csrf_key_css).extract_first()

        # Load threads and stamps
        all_threads = response.css(self.thread_url_css).extract()
        all_stamps = response.css(self.thread_stamp_css).extract()
        if not all_threads or not all_stamps:
            self.logger.info(
                "Found no more thread, exit!"
            )
            return

        # Loop threads
        for thread in all_threads:
            yield Request(
                url=thread,
                headers=self.headers,
                callback=self.parse_topic
            )

        # Load payload
        payload = self.pagination_payload.copy()
        payload.update(
            {
                "csrfKey": csrf_key,
                "before": all_stamps[-1]
            }
        )
        yield Request(
            url=self.unread_api,
            headers=self.post_headers,
            body=urlencode(payload),
            method="POST",
            dont_filter=True,
            meta={
                "csrf_key": csrf_key
            },
            callback=self.parse_pagination
        )

    def parse_pagination(self, response):

        # Load csrf key
        csrf_key = response.meta.get("csrf_key")

        # Load json data
        json_data = json.loads(response.text)

        # Load html data
        html_data = json_data.get("results")
        if not html_data:
            self.logger.info(
                "No more topic."
            )
            return

        # Do pagination
        yield from self.parse_thread(
            Selector(text=html_data),
            csrf_key=csrf_key
        )

    def parse_topic(self, response):
        # Load topic id and page
        topic_id = self.get_topic_id.search(
            response.request.url
        ).group()
        current_page = int(
            (response.css(self.current_page_css).extract_first()
             or 1)
        )
        self.logger.info(
            "Parsing topic: %s, page: %s" % (topic_id, current_page)
        )

        # Write content
        with open(
            file=os.path.join(
                self.output_path,
                "%s-%s.html" % (
                    topic_id,
                    current_page
                )
            ),
            mode="w+",
            encoding="utf-8"
        ) as file:
            file.write(response.text)

        # Pagination
        next_page = response.css(self.next_page_css).extract_first()
        if not next_page:
            self.logger.info(
                "Last page of topic %s is %s" % (
                    topic_id,
                    current_page)
            )
            return
        yield Request(
            url=next_page,
            headers=self.headers,
            callback=self.parse_topic
        )


class ExploitScrapper(FromDateScrapper):

    from_date_spider_class = ExploitFromDateSpider

    # Url stuffs
    login_url = "https://forum.exploit.in/index.php?act=Login&CODE=01"
    topic_url = "https://forum.exploit.in/index.php?showtopic={}"

    # Regex pattern
    avatar_name_pattern = re.compile(r'.*/(\w+\.\w+)')

    # Xpath stuffs
    ignore_xpath = '//div[@class="errorwrap"]'

    def __init__(self, kwargs):
        super().__init__(kwargs)
        self.username = kwargs.get('user')
        self.password = kwargs.get('password')

    def login(self):
        if not self.username:
            self.username = USERNAME
        if not self.password:
            self.password = PASSWORD
        payload = {
            'UserName': self.username,
            'PassWord': self.password,
            "CookieDate": "1"
        }
        login_response = self.session.post(self.login_url, data=payload)
        html_response = self.get_html_response(login_response.content)
        if html_response.xpath('//div[@class="errorwrap"]'):
            return False
        return True

    def write_paginated_data(self, html_response):
        next_page_block = html_response.xpath(
            '//span[@class="pagecurrent"]'
            '/following-sibling::span[1]/a/@href'
        )
        if not next_page_block:
            return
        next_page_url = next_page_block[0]
        pattern = re.compile(r'showtopic=(\d+)&st=(\d+)')
        match = pattern.findall(next_page_url)
        if not match:
            return
        topic, pagination_value = match[0]

        content = self.get_page_content(
            next_page_url, self.ignore_xpath
        )
        if not content:
            return

        paginated_file = '{}/{}-{}.html'.format(
            self.output_path, topic, pagination_value
        )
        with open(paginated_file, 'wb') as f:
            f.write(content)

        print('{}-{} done..!'.format(topic, pagination_value))
        return content

    def clear_cookies(self,):
        self.session.cookies['topicsread'] = ''

    def get_avatar_info(self, html_response):
        avatar_info = dict()
        # Need to change xpath since exploit is down now.
        urls = html_response.xpath(
            '//div[@class="uix_avatarHolderInner"]/a/img/@src'
        )
        for url in urls:
            if self.site_link not in url:
                url = self.site_link + url
            name_match = self.avatar_name_pattern.findall(url)
            if not name_match:
                continue
            name = name_match[0]
            if name not in avatar_info:
                avatar_info.update({
                    name: url
                })
        return avatar_info

    def process_topic(self, topic):
        try:
            response = self.process_first_page(
                topic, self.ignore_xpath
            )
            if response is None:
                return

            avatar_info = self.get_avatar_info(response)
            for name, url in avatar_info.items():
                self.save_avatar(name, url)

            # ------------clear cookies without logout--------------
            self.clear_cookies()
        except:
            traceback.print_exc()
            return
        self.process_pagination(response)

    def do_new_posts_scrape(self,):
        print('**************  New posts scan  **************')
        print('Implementation not complete yet!!')

    def do_rescan(self,):
        print('**************  Rescanning  **************')
        print('Broken Topics found')
        broken_topics = self.get_broken_file_topics()
        print(broken_topics)
        if not broken_topics:
            return
        if not self.login():
            print('Login failed! Exiting...')
            return
        print('Login Successful!')
        for topic in broken_topics:
            file_path = "{}/{}.html".format(self.output_path, topic)
            if os.path.exists(file_path):
                os.remove(file_path)
            self.process_topic(topic)

    def do_scrape_from_date(self):
        print('**************  Exploit Scrapper Scrape from date  **************\n')
        super().do_scrape_from_date()

    def do_scrape(self):
        print('**************  Exploit Scrapper Started  **************\n')
        if not self.login():
            print('Login failed! Exiting...')
            return
        print('Login Successful!')
        # ----------------go to topic ------------------
        ts = self.topic_start_count or TOPIC_START_COUNT
        te = self.topic_end_count or TOPIC_END_COUNT + 1
        topic_list = list(range(ts, te))
        # random.shuffle(topic_list)
        for topic in topic_list:
            self.process_topic(topic)


def main():
    template = ExploitScrapper()
    template.do_scrape()


if __name__ == '__main__':
    main()
