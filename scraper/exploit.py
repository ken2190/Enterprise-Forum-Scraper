import os
import re
import scrapy
import datetime
import dateutil.parser as dparser
import uuid

from datetime import datetime
from urllib.parse import urlencode
from scrapy.http import Request, FormRequest
from scrapy.crawler import CrawlerProcess, Crawler
from scrapy.exceptions import CloseSpider
from scraper.base_scrapper import SitemapSpider, SiteMapScrapper

# Credentials
USERNAME = "x23"
PASSWORD = "yoENbWrYjxnu7qhCu"
FIRST_LVL_PASSWORD = "F4Az0l7T3gUzwQ2"

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'

class ExploitSpider(SitemapSpider):

    name = 'exploit_spider'

    # Url stuffs
    base_url = "https://forum.exploit.in"

    # Css stuffs
    # Xpath stuffs
    forum_xpath = '//div[@id="ipsLayout_mainArea"]//h4[contains(@class, "ipsDataItem_title ipsType_large")]/a/@href'
    login_form_xpath = '//form[@class="ipsBox_alt"]'

    pagination_xpath = '//li[@class="ipsPagination_next"]/a/@href'

    thread_xpath = '//div[@id="ipsLayout_mainArea"]//div[@class="ipsBox"]/ol/li[contains(@class, "ipsDataItem ipsDataItem_responsivePhoto")]'
    thread_first_page_xpath = './div[@class="ipsDataItem_main"]/h4//a/@href'
    thread_last_page_xpath = './div[@class="ipsDataItem_main"]/h4//span[contains(@class, "ipsPagination_mini")]/span[last()]/a/@href'
    thread_date_xpath = './ul[contains(@class, "ipsDataItem_lastPoster")]//time/@datetime'

    thread_pagination_xpath = '//ul[@class="ipsPagination"]//li[@class="ipsPagination_prev"]/a/@href'
    thread_page_xpath = '//ul[@class="ipsPagination"]/li[contains(@class, "ipsPagination_active")]/a/text()'
    post_date_xpath = '//div[@class="ipsType_reset"]//time/@datetime'

    avatar_xpath = '//div[@class="cAuthorPane_photo"]//img/@src'

    use_proxy = 'On'
    # Regex stuffs
    avatar_name_pattern = re.compile(
        r".*/(\S+\.\w+)",
        re.IGNORECASE
    )
    pagination_pattern = re.compile(
        r".*page/(\d+)",
        re.IGNORECASE
    )

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.headers.update(
            {
                "User-Agent": USER_AGENT
            }
        )

    def start_requests(self):
        cookies, ip = self.get_cookies(
            base_url=self.base_url,
            proxy=self.use_proxy,
            fraud_check=True,
        )

        self.logger.info(f'COOKIES: {cookies}')

        # Init request kwargs and meta
        meta = {
            "cookiejar": uuid.uuid1().hex,
            "ip": ip
        }

        yield Request(
            url=self.base_url,
            headers=self.headers,
            meta=meta,
            cookies=cookies,
            callback=self.parse_start
        )

    def get_cookies_extra(self, browser):
        def find_element(name):
            for element in browser.find_elements_by_name(name):
                if element.is_displayed():
                    return element
        try:
            find_element('auth').send_keys(USERNAME)
            password_field = find_element('password')
            password_field.send_keys(PASSWORD)
            find_element('_processLogin').click()
        except:
            return False

        # Check if login success
        return USERNAME.lower() in browser.page_source.lower()

    def check_if_logged_in(self, response):
        # check if logged in successfully
        if self.login_failed_xpath:
            login_failed = response.xpath(self.login_failed_xpath).extract()
            if len(login_failed):
                raise CloseSpider(reason='login_is_failed')

    def parse_start(self, response):

        # Synchronize user agent for cloudfare middleware
        self.synchronize_headers(response)

        # Load all forums
        all_forums = response.xpath(self.forum_xpath).extract()

        # update stats
        self.crawler.stats.set_value("mainlist/mainlist_count", len(all_forums))
        
        for forum_url in all_forums:
            # Standardize forum url
            if self.base_url not in forum_url:
                forum_url = response.urljoin(forum_url)

            yield Request(
                url=forum_url,
                headers=self.headers,
                meta=self.synchronize_meta(response),
                callback=self.parse_forum
            )

    def parse_forum(self, response,is_first_page=True):

        # Parse sub forums
        yield from self.parse(response)

        # Parse generic forum
        yield from super().parse_forum(response)

    def parse_thread(self, response):

        # Save generic thread
        yield from super().parse_thread(response)

        # Save avatars
        yield from super().parse_avatars(response)


class ExploitScrapper(SiteMapScrapper):

    spider_class = ExploitSpider
    site_name = 'Exploit (exploit.in)'
    site_type = 'forum'

    def load_settings(self):
        settings = super().load_settings()
        settings.update({
            'RETRY_HTTP_CODES': [403]
        })
        return settings