import os
import re
import scrapy
import datetime
import sqlite3
import dateutil.parser as dparser

from datetime import datetime
from urllib.parse import urlencode
from scrapy.http import Request, FormRequest
from scrapy.crawler import CrawlerProcess
from scraper.base_scrapper import SiteMapScrapper

# Credentials
USERNAME = "x23"
PASSWORD = "y0Ir#vNx81k5d850&0kV"
FIRST_LVL_PASSWORD = "F4Az0l7T3gUzwQ2"


class ExploitSpider(scrapy.Spider):

    name = 'exploit_spider'

    # Url stuffs
    start_urls = ["https://forum.exploit.in/login"]
    unread_url = "https://forum.exploit.in/discover/unread/?&stream_read=all&" \
                 "stream_date_type=custom&stream_date_start=%s&stream_date_end=%s&" \
                 "stream_classes[IPS\\forums\\Topic]=1"
    unread_api = "https://forum.exploit.in/discover/unread/"

    # Payload stuffs
    post_headers = {
        "Content-Type": "application/x-www-form-urlencoded"
    }
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0"
    }
    pagination_payload = {
        "csrfKey": "",
        "before": "",
        "stream_read": "all",
        "stream_classes[IPS\\forums\Topic]": "Topics"
    }
    secret_payload = {
        "forum_password_submitted": "1",
        "csrfKey": "",
        "password": FIRST_LVL_PASSWORD
    }

    # Css stuffs
    thread_url_css = r".ipsType_break>a[href*=topic]::attr(href)"
    thread_stamp_css = r".ipsStreamItem::attr(data-timestamp)"
    csrf_key_css = r"[name=csrfKey]::attr(value)"
    current_page_css = r".ipsPagination_active>a::text"
    next_page_css = r".ipsPagination_next>a::attr(href)"

    # Xpath stuffs
    csrf_key_xpath = "//li[@data-menuitem=\"signout\"]/a/@href"
    forum1_xpath = "//h4/a[contains(@href, \"forum.exploit.in/forum/\")]"
    forum2_xpath = "//li/a[contains(@href, \"forum.exploit.in/forum/\")]"

    # Regex stuffs
    topic_pattern = re.compile(r'.*topic/(\d+)')
    avatar_name_pattern = re.compile(r'.*/(\S+\.\w+)')
    pagination_pattern = re.compile(r'.*page=(\d+)')

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # Load parameters
        self.output_path = kwargs.get("output_path")
        self.useronly = kwargs.get("useronly")
        self.avatar_path = kwargs.get("avatar_path")
        self.start_date = kwargs.get("start_date")

        # Update headers
        self.post_headers.update(self.headers)

    def get_topic_id(self, url):
        try:
            return self.topic_pattern.findall(url)[0]
        except Exception as err:
            return

    def parse(self, response):
        yield FormRequest.from_response(
            response=response,
            formdata={
                "auth": USERNAME,
                "password": PASSWORD
            },
            headers=self.post_headers,
            callback=self.parse_redirect
        )

    def parse_redirect(self, response):
        if self.start_date:
            yield from self.parse_update(response)
        else:
            yield from self.parse_begin(response)

    def parse_update(self, response):
        yield Request(
            url=self.unread_url % (
                str(int(self.start_date.timestamp())),
                str(int(datetime.now().timestamp()))
            ),
            headers=self.headers,
            callback=self.parse_stream
        )

    def parse_stream(self, response):
        # Load csrf key
        csrf_key = response.css(self.csrf_key_css).extract_first()

        # Load threads and stamps
        all_threads = response.css(self.thread_url_css).extract()
        all_stamps = response.css(self.thread_stamp_css).extract()

        if not all_threads or not all_stamps:
            self.logger.info(
                "Found no more thread, exit!"
            )
            return

        # If first stamp less than start date, quit
        if int(all_stamps[0]) < int(self.start_date.timestamp()):
            self.logger.info(
                "Those post is before %s. Exit!" % self.start_date
            )
            return

        # Loop threads
        for thread in all_threads:
            topic_id = self.get_topic_id(thread)
            if not topic_id:
                continue
            yield Request(
                url=thread,
                headers=self.headers,
                callback=self.parse_thread,
                meta={
                    "topic_id": topic_id
                }
            )

        # Load payload
        payload = self.pagination_payload.copy()
        payload.update(
            {
                "csrfKey": csrf_key,
                "before": all_stamps[-1]
            }
        )
        yield Request(
            url=self.unread_api,
            headers=self.post_headers,
            body=urlencode(payload),
            method="POST",
            dont_filter=True,
            meta={
                "csrf_key": csrf_key
            },
            callback=self.parse_stream
        )

    def parse_begin(self, response):

        # Load csrf_key
        csrfKey = response.xpath(
            self.csrf_key_xpath
        ).re(r'csrfKey=(.*)')[0]

        # Load forums1
        forums1 = response.xpath(self.forum1_xpath)

        # Load forums2
        forums2 = response.xpath(self.forum2_xpath)

        # Merge xpath
        forums = forums1 + forums2

        for forum in forums:
            url = forum.xpath('@href').extract_first()

            if 'https://forum.exploit.in/forum/39/' in url:
                url += '?passForm=1&ajaxValidate=1'

                payload = self.secret_payload.copy()
                payload["csrfKey"] = csrfKey

                yield FormRequest(
                    url=url,
                    callback=self.parse_forum,
                    formdata=payload
                )
            else:
                yield Request(
                    url=url,
                    callback=self.parse_forum,
                )

    def parse_forum(self, response):
        self.logger.info(
            "Next_page_url: %s" % response.url
        )

        # Load all threads
        all_threads = response.css(self.thread_url_css).extract()
        for thread_url in all_threads:

            # Load topic id
            topic_id = self.get_topic_id(thread_url)
            if not topic_id:
                continue

            # Yield content of topic
            file_name = '{}/{}-1.html'.format(self.output_path, topic_id)
            if os.path.exists(file_name):
                continue
            yield Request(
                url=thread_url,
                callback=self.parse_thread,
                meta={'topic_id': topic_id}
            )

        # Pagination
        next_page = response.css(self.next_page_css).extract_first()
        if not next_page:
            self.logger.info(
                "Forum %s end pagination." % response.request.url
            )
            return

        yield Request(
            url=next_page,
            callback=self.parse_forum,
        )

    def parse_thread(self, response):

        # Load topic id
        topic_id = response.meta['topic_id']

        # Load pagination
        pagination = self.pagination_pattern.findall(response.url)
        paginated_value = pagination[0] if pagination else 1

        # Load file name
        file_name = '%s/%s-%s.html' % (
            self.output_path,
            topic_id,
            paginated_value
        )

        # Save content
        with open(
            file=file_name,
            mode='w+',
            encoding="utf-8"
        ) as file:
            file.write(response.text)
            self.logger.info(
                f'{topic_id}-{paginated_value} done..!'
            )
        avatars = response.xpath(
            '//li[@class="cAuthorPane_photo"]/a/img')
        for avatar in avatars:
            avatar_url = avatar.xpath('@src').extract_first()
            if 'svg+xml' in avatar_url:
                continue
            if not avatar_url.startswith('https'):
                avatar_url = f'https:{avatar_url}'
            name_match = self.avatar_name_pattern.findall(avatar_url)
            if not name_match:
                continue
            name = name_match[0]
            file_name = '{}/{}'.format(self.avatar_path, name)
            if os.path.exists(file_name):
                continue
            yield Request(
                url=avatar_url,
                callback=self.parse_avatar,
                meta={
                    'file_name': file_name,
                }
            )

        # Pagination
        next_page = response.css(self.next_page_css).extract_first()
        if not next_page:
            self.logger.info(
                "Topic %s end pagination." % topic_id
            )
            return

        yield Request(
            url=next_page,
            callback=self.parse_thread,
            meta={
                'topic_id': topic_id
            }
        )

    def parse_avatar(self, response):
        file_name = response.meta['file_name']
        file_name_only = file_name.rsplit('/', 1)[-1]
        with open(file_name, 'wb') as f:
            f.write(response.body)
            self.logger.info(f"Avatar {file_name_only} done..!")


class ExploitScrapper(SiteMapScrapper):

    request_delay = 0.1
    no_of_threads = 16

    spider_class = ExploitSpider

    def load_settings(self):
        spider_settings = super().load_settings()
        spider_settings.update(
            {
                'DOWNLOAD_DELAY': self.request_delay,
                'CONCURRENT_REQUESTS': self.no_of_threads,
                'CONCURRENT_REQUESTS_PER_DOMAIN': self.no_of_threads
            }
        )


if __name__ == '__main__':
    run_spider('/Users/PathakUmesh/Desktop/BlackHatWorld')
