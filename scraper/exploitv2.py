import os
import re
import scrapy
from math import ceil
import configparser
from scrapy.http import Request, FormRequest
from scrapy.crawler import CrawlerProcess


# Credentials
USERNAME = "x23"
PASSWORD = "y0Ir#vNx81k5d850&0kV"
FIRST_LVL_PASSWORD = "F4Az0l7T3gUzwQ2"


class ExploitSpider(scrapy.Spider):
    name = 'exploit_spider'

    def __init__(self, output_path):
        self.login_url = "https://forum.exploit.in/login/"
        self.topic_pattern = re.compile(r'.*topic/(\d+)')
        self.avatar_name_pattern = re.compile(r'.*/(\S+\.\w+)')
        self.pagination_pattern = re.compile(r'.*page=(\d+)')
        self.output_path = output_path

    def start_requests(self):
        yield Request(
            url=self.login_url,
            callback=self.process_login,
        )

    def process_login(self, response):
        csrfKey = response.xpath(
            '//input[@name="csrfKey"]/@value').extract_first()
        ref = response.xpath(
            '//input[@name="ref"]/@value').extract_first()
        formdata = {
            'csrfKey': csrfKey,
            'ref': ref,
            'auth': USERNAME,
            'password': PASSWORD,
            'remember_me': '1',
            '_processLogin': 'usernamepassword',
            '_processLogin': 'usernamepassword',
        }
        yield FormRequest(
            url=self.login_url,
            callback=self.parse,
            formdata=formdata,
        )

    def parse(self, response):
        csrfKey = response.xpath(
            '//li[@data-menuitem="signout"]/a/@href').re(r'csrfKey=(.*)')[0]
        forums1 = response.xpath(
            '//h4/a[contains(@href, "forum.exploit.in/forum/")]')
        forums2 = response.xpath(
            '//li/a[contains(@href, "forum.exploit.in/forum/")]')
        forums = forums1 + forums2
        for forum in forums:
            url = forum.xpath('@href').extract_first()

            if 'https://forum.exploit.in/forum/39/' in url:
                url += '?passForm=1&ajaxValidate=1'
                formdata = {
                    'forum_password_submitted': '1',
                    'csrfKey': csrfKey,
                    'password': FIRST_LVL_PASSWORD
                }
                yield FormRequest(
                    url=url,
                    callback=self.parse_forum,
                    formdata=formdata
                )
            else:
                yield Request(
                    url=url,
                    callback=self.parse_forum,
                )

    def parse_forum(self, response):
        print('next_page_url: {}'.format(response.url))
        threads = response.xpath(
            '//span[@class="ipsType_break ipsContained"]/a')
        for thread in threads:
            thread_url = thread.xpath('@href').extract_first()
            topic_id = self.topic_pattern.findall(thread_url)
            if not topic_id:
                continue
            file_name = '{}/{}-1.html'.format(self.output_path, topic_id[0])
            if os.path.exists(file_name):
                continue
            yield Request(
                url=thread_url,
                callback=self.parse_thread,
                meta={'topic_id': topic_id[0]}
            )

        next_page = response.xpath('//li[@class="ipsPagination_next"]/a')
        if next_page:
            next_page_url = next_page.xpath('@href').extract_first()
            yield Request(
                url=next_page_url,
                callback=self.parse_forum,
            )

    def parse_thread(self, response):
        topic_id = response.meta['topic_id']
        pagination = self.pagination_pattern.findall(response.url)
        paginated_value = pagination[0] if pagination else 1
        file_name = '{}/{}-{}.html'.format(
            self.output_path, topic_id, paginated_value)
        with open(file_name, 'wb') as f:
            f.write(response.text.encode('utf-8'))
            print(f'{topic_id}-{paginated_value} done..!')

        next_page = response.xpath('//li[@class="ipsPagination_next"]/a')
        if next_page:
            next_page_url = next_page.xpath('@href').extract_first()
            yield Request(
                url=next_page_url,
                callback=self.parse_thread,
                meta={'topic_id': topic_id}
            )


class ExploitV2Scrapper():
    def __init__(self, kwargs):
        self.output_path = kwargs.get('output')
        self.proxy = kwargs.get('proxy') or None
        self.request_delay = 0.1
        self.no_of_threads = 16

    def do_scrape(self):
        settings = {
            'DOWNLOAD_DELAY': self.request_delay,
            'CONCURRENT_REQUESTS': self.no_of_threads,
            'CONCURRENT_REQUESTS_PER_DOMAIN': self.no_of_threads,
            'RETRY_HTTP_CODES': [403, 429, 500, 503],
            'RETRY_TIMES': 10,
            'LOG_ENABLED': True,

        }
        if self.proxy:
            settings.update({
                "DOWNLOADER_MIDDLEWARES": {
                    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
                    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
                    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,
                    'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,
                    'rotating_proxies.middlewares.BanDetectionMiddleware': 620,
                },
                'ROTATING_PROXY_LIST': self.proxy,

            })
        process = CrawlerProcess(settings)
        process.crawl(ExploitSpider, self.output_path)
        process.start()

if __name__ == '__main__':
    run_spider('/Users/PathakUmesh/Desktop/BlackHatWorld')
